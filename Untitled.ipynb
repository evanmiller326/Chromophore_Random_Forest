{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evanmiller/miniconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import ml_helpers as mlh\n",
    "\n",
    "from random_forest import plot_actual_vs_predicted as pavp\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "import ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "def teach_the_terminator(train_features, \n",
    "                         train_labels,\n",
    "                        ml_method):\n",
    "    \n",
    "    if ml_method == \"Ordinary Least Squares\":\n",
    "        reg = linear_model.LinearRegression()\n",
    "    if ml_method == \"Ridge Regression\":\n",
    "        reg = linear_model.Ridge()\n",
    "    if ml_method == \"Bayesian Regression\":\n",
    "        reg = linear_model.BayesianRidge()\n",
    "    if ml_method == \"Support Vector Machine\":\n",
    "        reg = SVR(kernel='rbf', max_iter = 1000)\n",
    "    if ml_method == \"Stochastic Gradient Descent\":\n",
    "        reg = linear_model.SGDRegressor()\n",
    "    if ml_method == \"K-Nearest Neighbors\":\n",
    "        reg = KNeighborsRegressor()\n",
    "    if ml_method == \"Random Forest\":\n",
    "        reg = RandomForestRegressor()\n",
    "    if ml_method == \"Gradient Boosting\":\n",
    "        reg = GradientBoostingRegressor()\n",
    "    reg.fit(train_features, train_labels)\n",
    "    return reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def run_net(train_features, \n",
    "#            test_features, \n",
    "#            train_labels, \n",
    "#            test_labels,\n",
    "#        Nlayers = 1, \n",
    "#        N_nodes= [1], \n",
    "#        training_iterations = 1e3, \n",
    "#        run_name = \"\", \n",
    "#        show_comparison = False, \n",
    "#        nfilters = 27,\n",
    "#        convolution_outsize = 9,\n",
    "#        forward_hops_only = False):\n",
    "#\n",
    "#    test_answers = test_labels\n",
    "#    train_features, test_features, train_labels, test_labels = train_features.values, test_features.values, train_labels.values, test_labels.values\n",
    "#\n",
    "#    assert Nlayers == len(N_nodes)\n",
    "#\n",
    "#    x = tf.placeholder(tf.float32, shape = [None, len(train_features[0])])\n",
    "#    y_ = tf.placeholder(tf.float32, shape = [None, 1])\n",
    "#\n",
    "#    layer_dict = {}\n",
    "#\n",
    "#    xconv = ANN.convolve(x, nfilters, convolution_outsize)\n",
    "#\n",
    "#    for N in range(Nlayers):\n",
    "#        if N == 0:\n",
    "#            layer_dict[\"layer_{}\".format(N)] = tf.nn.elu(ANN.weights(x, convolution_outsize, N_nodes[N]))\n",
    "#        elif N + 1 == Nlayers:\n",
    "#            layer_dict[\"layer_{}\".format(N)] = tf.nn.elu(ANN.weights(layer_dict[\"layer_{}\".format(N-1)], N_nodes[N-1], N_nodes[N]))\n",
    "#        else:\n",
    "#            layer_dict[\"layer_{}\".format(N)] = tf.nn.elu(ANN.weights(layer_dict[\"layer_{}\".format(N-1)], N_nodes[N-1], N_nodes[N]))\n",
    "#\n",
    "#    y_out = layer_dict[\"layer_{}\".format(Nlayers-1)]\n",
    "#\n",
    "#    #cost = tf.losses.huber_loss(labels = y_, predictions = y_out)\n",
    "#    cost = tf.losses.mean_squared_error(labels = y_, predictions = y_out)\n",
    "#\n",
    "#    #training_step = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n",
    "#\n",
    "#    optimizer = tf.train.AdamOptimizer(0.01)\n",
    "#    training_step = optimizer.minimize(cost)\n",
    "#\n",
    "#    session = tf.Session()\n",
    "#    session.run(tf.global_variables_initializer())\n",
    "#\n",
    "#    saver = tf.train.Saver()\n",
    "#\n",
    "#    training_error = []\n",
    "#\n",
    "#    for _ in range(int(training_iterations)):\n",
    "#        batch_vectors, batch_answers = ANN.get_batch(train_features, train_labels)\n",
    "#        session.run(training_step,feed_dict={x:batch_vectors,y_:batch_answers})\n",
    "#        if _ % (int(training_iterations)//10) == 0:\n",
    "#\n",
    "#            train_error = session.run(cost,feed_dict={x:train_features,y_:train_labels})\n",
    "#            training_error.append([_, train_error])\n",
    "#            print(train_error)\n",
    "#\n",
    "#    pred_y = session.run(y_out, feed_dict={x: test_features})\n",
    "#\n",
    "#    #rmse = session.run(cost,feed_dict={x:test_features,y_:test_labels})\n",
    "#    mae = np.mean(abs(pred_y-test_labels))\n",
    "#\n",
    "#    slope, intercept, r_value, p_value, std_err = stats.linregress(\n",
    "#        test_labels.flatten(), pred_y.flatten()\n",
    "#    )\n",
    "#\n",
    "#    plot_based_on_density(test_answers, pred_y, \"Neural Net\", r_value**2, mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_predictions(predictions, test_labels):\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(test_labels.values.flatten(), \n",
    "                                                                   predictions.flatten())\n",
    "    mae = np.mean(abs(predictions.flatten() - test_labels.values.flatten()))\n",
    "    return r_value, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib import cm\n",
    "\n",
    "def plot_based_on_density(actual, predicted, system, r_value, mae):\n",
    "    print(\"Beginning the Plot.\")\n",
    "    actual = np.array(actual.values).flatten()\n",
    "    predicted = predicted.flatten()\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    plt.hist2d(actual, predicted, (50, 50), norm=LogNorm(), cmap='viridis')\n",
    "    plt.colorbar()\n",
    "    plt.plot(np.linspace(0, np.amax(actual), 10),\n",
    "            np.linspace(0, np.amax(actual), 10),\n",
    "            'w--',\n",
    "            alpha = 0.5)\n",
    "    plt.xlabel(\"Actual (eV)\")\n",
    "    plt.ylabel(\"Predicted (eV)\")\n",
    "    plt.title(\"{},\".format(system)+r\"R$^{2}$:\"+\"{:.2f}, MAE:{:.3f}\".format(r_value**2, mae))\n",
    "    rgba = cm.get_cmap('viridis')\n",
    "    rgba = rgba(0.0)\n",
    "    ax.set_facecolor(rgba)\n",
    "    ax.set_xlim([0, np.amax(actual)])\n",
    "    ax.set_ylim([0, np.amax(predicted)])\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def manual_load(database=\"p3ht_pdi.db\", training_tables=None, validation_tables=None,\n",
    "#             absolute=None, skip=[], yval=\"TI\"):\n",
    "#    training_records = []\n",
    "#    print(\"\".join([\"Loading data from \", database, \"...\"]))\n",
    "#    # Obtain training tables first:\n",
    "#    if training_tables is None:\n",
    "#        # Fetch all tables\n",
    "#        print(\"Using all tables to train from...\")\n",
    "#        connection = sqlite3.connect(database)\n",
    "#        cursor = connection.cursor()\n",
    "#        query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "#        cursor.execute(query)\n",
    "#        training_tables = [name[0] for name in cursor.fetchall()]\n",
    "#        cursor.close()\n",
    "#        connection.close()\n",
    "#    for table_name in training_tables:\n",
    "#        data, all_column_names = mlh.load_table(database, table_name)\n",
    "#        for record in data:\n",
    "#            training_records.append(record)\n",
    "#    column_names_to_use = list(set(all_column_names) - set(skip))\n",
    "#    train_features, train_labels = mlh.create_data_frames(np.array(training_records),\n",
    "#                                                      absolute,\n",
    "#                                                      all_column_names,\n",
    "#                                                      column_names_to_use,\n",
    "#                                                      yval)\n",
    "#    print(\"Separating training and test data...\")\n",
    "#    if validation_tables is None:\n",
    "#        # Split the dataset we have to be 95%:5%\n",
    "#        return train_test_split(train_features, train_labels, test_size=0.05)\n",
    "#    else:\n",
    "#        # Need to load the right validation data:\n",
    "#        validation_records = []\n",
    "#        for table_name in validation_tables:\n",
    "#            data, _ = mlh.load_table(database, table_name)\n",
    "#            for record in data:\n",
    "#                validation_records.append(record)\n",
    "#        test_features, test_labels = mlh.create_data_frames(np.array(validation_records),\n",
    "#                                                        absolute,\n",
    "#                                                        all_column_names,\n",
    "#                                                        column_names_to_use,\n",
    "#                                                        yval)\n",
    "#\n",
    "#    test_features.sort_index(axis=1, inplace=True)\n",
    "#    train_features.sort_index(axis=1, inplace=True)\n",
    "#\n",
    "#    return train_features, test_features, train_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def apply_machine_method(method_name, results_df, plot_scatter=False):\n",
    "#\n",
    "#    df_train = pd.concat([train_features, train_labels], axis=1)\n",
    "#    df_test = pd.concat([test_features, test_labels], axis=1)\n",
    "#\n",
    "#    # Stitch the training and testing data by concatenating along\n",
    "#    # axis = 0\n",
    "#    #df = pd.concat([df_train, df_test], axis=0)\n",
    "#\n",
    "#    reg = teach_the_terminator(train_features, train_labels, method_name)\n",
    "#\n",
    "#    predictions = np.array([reg.predict(test_features)]).T \n",
    "#    \n",
    "#    if len(predictions.shape) == 3:\n",
    "#        predictions = predictions[0]\n",
    "#        \n",
    "#    results_df[method_name] = predictions\n",
    "#\n",
    "#    r_value, mae = analyze_predictions(predictions, test_labels)\n",
    "#\n",
    "#    plot_based_on_density(test_labels, predictions, method_name, r_value, mae)\n",
    "#    if plot_scatter:\n",
    "#        pavp(test_labels, predictions, r_value, mae, name=method_name)\n",
    "#    \n",
    "#    del reg\n",
    "#    \n",
    "#    return results_df\n",
    "\n",
    "def apply_combined(method_name, trainf, testf, trainl, testl,plot_scatter=False):\n",
    "    \n",
    "    reg = teach_the_terminator(trainf, \n",
    "                               trainl, \n",
    "                               method_name)\n",
    "    predictions = np.array([reg.predict(testf).T])\n",
    "    \n",
    "    if len(predictions.shape) == 3:\n",
    "        predictions = predictions[0]\n",
    "        \n",
    "    r_value, mae = analyze_predictions(predictions.T, testl.T)\n",
    "\n",
    "    plot_based_on_density(testl, predictions, method_name, r_value, mae)\n",
    "    if plot_scatter:\n",
    "        pavp(test_labels, predictions, r_value, mae, name=method_name)\n",
    "    \n",
    "    del reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from p3ht_pdi.db...\n",
      "Separating training and test data...\n"
     ]
    }
   ],
   "source": [
    "train_features, test_features, train_labels, test_labels = mlh.get_data(\n",
    "    database = 'p3ht_pdi.db',\n",
    "    training_tables=['disordered_frame_0', \n",
    "                     'semiordered_frame_0',\n",
    "                    'ordered_frame_0'],\n",
    "    validation_tables=['disordered_frame_1'],\n",
    "    absolute=['rotX', 'rotY', 'rotZ'],\n",
    "    skip=['delta_E', 'chromophoreA', 'chromophoreB'],\n",
    "    yval='TI',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordinary Least Squares, Ridge Regression and Bayesian Regression all produce the same predictions.\n",
    "\n",
    "Using the Support Vector Machine on the complete data set was too slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(10-8-18)\n",
    "\n",
    "Without doing any normalizing and using the built in scikit learn functions, the random forest and K-Nearest neighbors are the only two that give decent regression predictions. Support vector machines start having issues with the large dataset, but can be aleviated by setting a number of max iterations. However, it predicts a high number of actual 0 TIs with about 0.25 predicted TI. However, the bonded peak is fairly close to what's predicted.\n",
    "\n",
    "In using the KNN and RF, the RF does a better job at fitting the data in that, the bonded atoms have a tail going towards the origin\n",
    "\n",
    "(10-20-18)\n",
    "\n",
    "I've split up the training into two portions: ''bonded'' and ''non-bonded''. This still causes some error because while labeled bonded, it is acutally the same chain that is being tested. As such there is likely wrapping that occurs and monomers monomers are neighbors but not actually bonded and should have low TIs. Need to test, actually bonded to verify this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(10-26-18)\n",
    "\n",
    "The systems lose the Actual 0 values when the bonded/nonbonded are specified rather than the same chain. So this metric seems to do better. \n",
    "The neural net still struggles with the low values of the bonded/non-bonded training.\n",
    "\n",
    "Need to see if splitting it is what's needed or just having the better metric of same chain vs bonded.\n",
    "\n",
    "Having bonded and nonbonded combined seems to work just fine for the forest, which still has the best overall results.\n",
    "The ANN struggles with the small, non-bonded values making it less favorable. K-nearest works okay, but doesn't get the shape of the distribution as well as the forest. \n",
    "\n",
    "When running the forest on the externally curated data there is a difference from what's seen here (There are quite a few low predicted, acutal high values). So I'm going to trim the notebook and rerun it to see if memory not being cleared is contributing to the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['TI'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3695\u001b[0m                                            \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3696\u001b[0m                                            \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3697\u001b[0;31m                                            errors=errors)\n\u001b[0m\u001b[1;32m   3698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3699\u001b[0m     @rewrite_axis_style_signature('mapper', [('copy', True),\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3109\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3110\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3111\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3141\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3142\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3143\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3144\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   4402\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4403\u001b[0m                 raise KeyError(\n\u001b[0;32m-> 4404\u001b[0;31m                     '{} not found in axis'.format(labels[mask]))\n\u001b[0m\u001b[1;32m   4405\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4406\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['TI'] not found in axis\""
     ]
    }
   ],
   "source": [
    "%time apply_combined(\"Random Forest\", train_features.drop('TI'), test_features.drop('TI'), train_labels, test_labels,plot_scatter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['TI'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5069213f5230>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TI'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3695\u001b[0m                                            \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3696\u001b[0m                                            \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3697\u001b[0;31m                                            errors=errors)\n\u001b[0m\u001b[1;32m   3698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3699\u001b[0m     @rewrite_axis_style_signature('mapper', [('copy', True),\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3109\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3110\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3111\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3141\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3142\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3143\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3144\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   4402\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4403\u001b[0m                 raise KeyError(\n\u001b[0;32m-> 4404\u001b[0;31m                     '{} not found in axis'.format(labels[mask]))\n\u001b[0m\u001b[1;32m   4405\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4406\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['TI'] not found in axis\""
     ]
    }
   ],
   "source": [
    "train_features.drop(['TI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
